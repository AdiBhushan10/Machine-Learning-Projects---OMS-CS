import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve
import seaborn as sns
randomSeed = 11  #Dataset1 - 11 through 15

MyFrame = pd.read_csv('C:/Users/User2/Desktop/GATECH/Machine Learning/Datasets/Dataset1 - Assignment 1/ExoPlanetData.csv')
MyFrame['Detected_ExoPlanet'] = MyFrame['DispositionUsingKeplerData'].apply(lambda x: 1 if x == 'CANDIDATE' else 0)
MyFrame.drop(columns=['Object_Name','DispositionUsingKeplerData'], inplace=True)
MyFrame.fillna(MyFrame.median(), inplace=True)
print(MyFrame.shape)

# Independent features and target feature
features = MyFrame.drop(columns=['Detected_ExoPlanet'])
target = MyFrame['Detected_ExoPlanet']
print(target.value_counts())


#Test Train Split: 75-25; Train Validation Split: 60-40
X_train, X_test, Y_train, Y_test = train_test_split(features, target, random_state=randomSeed, test_size=0.25)
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, random_state=randomSeed, test_size=0.4)
print("Shape of validation set:", X_val.shape)
print("Shape of test set:", X_test.shape)
print("Shape of training set:", X_train.shape)

#Before Hyperparameter Tuning (No pruning)
from sklearn.tree import DecisionTreeClassifier
MyTree = DecisionTreeClassifier(random_state=randomSeed)
MyTree.fit(X_train,Y_train)
predicting = MyTree.predict(X_val)
from sklearn.metrics import f1_score
score_val=f1_score(Y_val,predicting)
print(f'Accuracy before hyperparameter tuning: {score_val}')

#With Pruning - Changing max_depth parameter 
#We will plot model complexity curve using 1 hyperparameter only (max_depth) and then use it in learning curve and check accuracy

estimator = DecisionTreeClassifier(random_state=randomSeed) 
train_score, test_score = validation_curve(
        estimator, X_train, Y_train,param_name= 'max_depth', param_range = np.arange(1, 11) ,cv = 4, n_jobs=1, scoring='f1')

# Store stats for testing score 
mean_test_score = np.mean(test_score, axis = 1) 
std_test_score = np.std(test_score, axis = 1) 
# Store stats for training score 
mean_train_score = np.mean(train_score, axis = 1) 
std_train_score = np.std(train_score, axis = 1) 
  
# Plotting training and cross-validation scores 
param_range = np.arange(1, 11)
plt.plot(param_range, mean_train_score,  
     label = "Training Score", color = 'brown') 
plt.plot(param_range, mean_test_score, 
   label = "Cross-Validation Score", color = 'green') 
title = "Model Complexity/Validation Curve: ExoPlanet Identification"
plt.title(title)
plt.xlabel("Max_depth")
plt.ylabel("Model Score")
plt.tight_layout() 
plt.legend(loc = 'best') 
plt.show()

# Hyperparameter Tuning begins
# GridSearch for best parameters
parameters = {
    'max_depth': list(range(1, 11))
    ,'criterion':['gini','entropy'] # we can comment it out if it takes more time, it will NOT change our scores significantly
    ,'min_samples_leaf': list(range(1, 11))
}
grid = GridSearchCV(DecisionTreeClassifier(random_state=randomSeed), param_grid=parameters, cv=4, n_jobs=-1, 
                      verbose=1, return_train_score=True, scoring='f1')
grid.fit(X_train, Y_train)
# Function Prints best parameters for GridSearchCV
print('Top Parameters: {}\n'.format(grid.best_params_))
print('Top Score: {}'.format(grid.best_score_))
print(f'This classification model has a top accuracy of {math.ceil(100*grid.best_score_)} % using grid search which is an improvement over initial score of {math.floor(100*score_val)} % on the validation dataset')

# Pruning Result
import copy
import sklearn.tree as tree
Complete_dt = copy.deepcopy(grid.best_estimator_)
Complete_dt.max_depth = 10 
Complete_dt.fit(X_train, Y_train)
print(f'The entire tree has {Complete_dt.tree_.node_count} nodes')
Pruned_dt = grid.best_estimator_
print(f'The pruned tree has {Pruned_dt.tree_.node_count} nodes')

# Learning Curve computation using the top parameters
val_set=[]
train_set=[]
for i in range(20,101,20):
    # Classifier Initialized
    percentage_of_total=i*0.01
    new_dt = DecisionTreeClassifier(random_state=randomSeed,**(grid.best_params_))
    # Sample Data
    df_samples = MyFrame.sample(frac=percentage_of_total)
    # Generate target
    X_train_samples=df_samples.drop('Detected_ExoPlanet',axis=1)
    Y_train_samples=df_samples['Detected_ExoPlanet']
    # Fit and Predict
    new_dt.fit(X_train_samples,Y_train_samples)
    predict_train = new_dt.predict(X_train_samples)
    predict_val = new_dt.predict(X_val)
    # F1-Accuracy score
    accr_train=f1_score(Y_train_samples,predict_train)
    accr_val=f1_score(Y_val,predict_val)
  
    train_set.append(accr_train)
    val_set.append(accr_val)

# Plotting Learning Curve
plt.plot(range(20,101,20),train_set,c='brown')
plt.plot(range(20,101,20),val_set,c='black')
title = "Learning Curve: ExoPlanet Identification"
plt.title(title)
plt.legend(["Training Scores","Validation Scores"])
plt.xlabel("Training Sample (in %)")
plt.ylabel("Model Accuracy")
plt.ylim(0.8,1.0)
plt.xticks(np.arange(0,120,step=20))
plt.show()

